---
title: 深度学习-基础知识
date: 2025-11-22 00:52:29
tags:
mathjax: true
---

# 拟合与泛化

## 过拟合 vs 欠拟合

- **过拟合**：训练集表现很好，但验证/测试集性能下降，说明模型记住了噪声或特例。
- **欠拟合**：训练集和验证集都表现糟糕，通常意味着模型容量不足或训练不充分。

## 如何判断

- 绘制训练 Loss 与验证 Loss 曲线。
  - 训练 Loss 持续下降而验证 Loss 上升 → 过拟合。
  - 两者都停留在高位 → 欠拟合。
- 对比训练/验证准确率或其他指标是否出现明显分叉。

## 缓解策略

- **应对过拟合**：
  - 收集更多数据或做增强（翻转、裁剪、颜色抖动、Mixup/CutMix 等）。
  - 增加正则化（L1/L2、Dropout、Label Smoothing、数据噪声）。
  - 降低模型复杂度、使用 Early Stopping、应用 BatchNorm。
- **应对欠拟合**：
  - 使用更大的模型或更强的结构。
  - 训练更久、采用更合适的学习率策略。
  - 降低正则化强度或改进特征。

# 数据准备与特征工程

## 数据集划分

- 典型拆分：训练集/验证集/测试集（例如 8/1/1），保证各子集分布一致。
- 数据量有限时可使用 k 折交叉验证轮流作为验证集。
- 保持随机种子与分层抽样，避免类别不平衡导致的偏差。

## 预处理与特征工程

- 数值特征常做标准化（Z-score）或归一化到固定区间，防止量纲影响梯度。
- 图像常做均值方差归一化、直方图均衡、白化等；文本需要分词/Tokenizer、构建词典、转换为 Embedding。
- 离散特征通过 one-hot、Embedding、目标编码等方式注入模型。

## 批处理与数据管线

- DataLoader 通常负责 shuffle、batch、并行加载与缓存，保证训练稳定。
- Prefetch、pin memory、mmap、TFRecord 之类技巧可提高吞吐。
- 在线数据增强（在读取时随机变换）能避免存储大量增强样本。

# 前向传播与反向传播

## 计算图

- 深度学习模型可视为由线性/非线性层组成的有向无环图，前向传播按拓扑顺序计算输出值。
- 常见操作：矩阵乘、卷积、逐元素激活、拼接、归一化等。

## 反向传播

- 基于链式法则：若 $z=f(y)$ 且 $y=g(x)$，则 $\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y}\cdot\frac{\partial y}{\partial x}$。
- 从损失对输出的梯度开始，沿计算图逆序乘以局部梯度即可得到所有参数的梯度。

## 自动微分

- 框架（PyTorch、TensorFlow、JAX）都会记录计算图并自动求导，开发者只需定义前向过程。
- 明确 requires_grad/stop_gradient、合理释放梯度（zero grad），可以避免显存泄漏。

# 正则化

## L1 与 L2

- **L1 正则**：在损失中加入 $\lambda\|w\|_1$，大量权重被压到 0，模型稀疏、利于特征选择。
- **L2 正则**：加入 $\lambda\|w\|_2^2$，权重整体收缩但不为 0，模型更平滑、稳定。
- **直观区别**：L1 "让很多参数干脆不用"；L2 "让所有参数都收一收"。

## 其他常见手段

- **Dropout**：训练时随机屏蔽部分神经元，相当于做子网络集成，减少共适应。
- **BatchNorm / LayerNorm**：稳定每层输入分布，允许更大学习率，并带来自然的正则化效果。
- **Label Smoothing**：将 one-hot 标签的 1 改成 $1-\epsilon$，其它类别分到 $\epsilon$，降低过度自信。
- **Early Stopping / Weight Decay**：监控验证集，当指标不再提升时提前停止；Weight Decay 与 L2 等价，常直接作用于优化器。

# 激活函数

常见激活函数的数学式与要点：

- **Sigmoid**：$\sigma(x)=\frac{1}{1+e^{-x}}$，输出 (0,1)，可解释为概率；缺点是梯度饱和。
- **Tanh**：$\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$，输出 (-1,1)，收敛快于 Sigmoid，但仍会饱和。
- **ReLU**：$\operatorname{ReLU}(x)=\max(0, x)$，简单高效但会出现 Dead ReLU。
- **Leaky ReLU**：
  $$
  \text{LeakyReLU}(x)=\begin{cases}
  x, & x\ge 0 \\
  \alpha x, & x<0
  \end{cases}
  $$
  其中 $\alpha\approx0.01$，缓解 Dead ReLU。
- **PReLU**：$\max(0,x)+a\min(0,x)$，负半轴斜率 $a$ 可学习。
- **ELU**：
  $$
  \text{ELU}(x)=\begin{cases}
  x, & x\ge 0 \\
  \alpha(e^x-1), & x<0
  \end{cases}
  $$
  让负半轴更平滑。
- **GELU**：$x\cdot\Phi(x)=x\cdot\frac{1}{2}\left(1+\operatorname{erf}\left(\frac{x}{\sqrt{2}}\right)\right)$，Transformer 中常见。
- **Swish**：$x\cdot\sigma(\beta x)$（常用 $\beta=1$），梯度更平滑，性能略优于 ReLU。

# 损失函数与指标

## 交叉熵

$$
\text{CE}(y,p)=-\sum_i y_i \log p_i
$$

- 标签为 one-hot 时等价于最大化正确类别的 log 概率。
- 与 softmax 组合后的梯度稳定、本质在最小化真实分布与预测分布的 KL 散度。

## 其他损失函数

- **MSE**：$\frac{1}{N}\sum_i (y_i-\hat y_i)^2$，对离群点敏感，常用于回归。
- **MAE**：$\frac{1}{N}\sum_i |y_i-\hat y_i|$，对离群点更鲁棒，但在 0 点不可导。
- **Huber**：在误差较小时为二次，大于阈值 $\delta$ 时转为线性，兼顾 MSE/MAE 优势。
- **Hinge / Multi-class Hinge**：$\max(0,1-y_i f(x_i))$，常用于支持向量机等最大间隔分类。
- **Focal Loss**：$-(1-p_t)^\gamma \log(p_t)$，$p_t$ 为真实类别概率，通过调节 $\gamma$ 抑制易分类样本，常用于检测/不平衡分类。

## 常见指标

- 分类：Accuracy、Precision、Recall、F1、ROC/AUC。
- 回归：MSE/MAE、RMSE、R²；MSE 对离群点敏感，MAE 更鲁棒但在 0 点不可导。

# 优化与训练策略

## Mini-Batch 必要性

- Full-batch 梯度最精确但慢且耗显存；纯 SGD (batch=1) 更新快却噪声大。
- Mini-batch 在效率与稳定性之间取折中，可利用 GPU 并行，梯度估计更平滑。

## 优化器

- **SGD**：沿负梯度方向更新。
- **Momentum**：引入动量项，积累之前的梯度方向，加速收敛并抑制震荡。
- **Adam**：同时估计梯度的一阶/二阶矩，为不同参数分配自适应学习率，前期收敛快，但泛化有时略弱于 SGD+Momentum，可在后期切换。

## 学习率调度

- Step/Exponential Decay：按固定间隔或指数下调。
- Cosine Annealing：富有周期感，可配合 Warmup。
- Warmup：训练初始从较小 LR 逐步升高，避免震荡。
- Cyclic / OneCycle：先升后降，在 CV、NLP 任务中常见。

## 梯度消失/爆炸

- 原因：深层链式相乘、激活饱和、初始化不当。
- 对策：使用 ReLU 家族、Xavier/He 初始化、残差结构、BatchNorm/LayerNorm、梯度裁剪。

# 典型网络模块

## CNN

- 卷积层具备参数共享、局部感受野、平移不变性三大优势。
- 卷积输出尺寸：输入 $H\times W$，核大小 $K$，padding $P$，stride $S$，则
  $$
  H_{out}=\left\lfloor\frac{H+2P-K}{S}\right\rfloor+1,
  \quad
  W_{out}=\left\lfloor\frac{W+2P-K}{S}\right\rfloor+1.
  $$
- Pooling 用于下采样和提升鲁棒性：Max Pool 保留最强响应，Avg Pool 更平滑，Global Avg Pool 常用于分类尾部。

## RNN / LSTM / GRU

- 普通 RNN 在长序列上易梯度消失/爆炸。
- LSTM 通过遗忘门、输入门、输出门维护细胞状态 $c_t$，缓解长期依赖问题。
- GRU 将细胞状态与隐状态合并，只保留更新门和重置门，参数更少、计算更快。

## Attention 与 Transformer

- Self-Attention：对每个位置 $i$ 计算其 Query 与其他位置 Key 的相似度，再对 Value 加权求和。
- Multi-Head：多组 $W_Q,W_K,W_V$ 并行，捕获不同关系。
- 由于注意力缺乏位置信息，需要添加可学习或正余弦位置编码。
- 核心公式：
  $$
  Attention(Q,K,V)=\text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V.
  $$
- Transformer 完全基于 Self-Attention，可并行处理序列，易于扩展到 GPT、BERT、ViT 等大模型。

# 归一化与正则细节

- **BatchNorm**：在通道维上对 mini-batch 标准化，再学习缩放/平移，提升收敛速度并具备轻微正则化；训练和推理需区分均值/方差的来源。
- **LayerNorm**：对同一样本的特征维做标准化，与 batch 大小无关，适合 Transformer/NLP。
- **Dropout** 与 **数据增强** 搭配使用可明显提升泛化。
- **权重初始化**：Xavier/Glorot 适合近似线性激活，He 初始化匹配 ReLU 家族；良好的初始化能避免一开始就梯度消失。


